"""
STM32N6 ONNX è¨ºæ–·å·¥å…·
æª¢æ¸¬ ONNX æ¨¡å‹æ˜¯å¦é•å STM32 Edge AI Core v2.2.0 çš„é™åˆ¶

ç”¨æ³•:
    python diagnose_onnx_stm32.py --onnx models/rppg_4d_fp32.onnx

æª¢æŸ¥é …ç›®:
    1. å¼µé‡ç¶­åº¦ (max rank = 5, no 6D)
    2. Batch å›ºå®š (batch = 1, no dynamic)
    3. Dynamic shapes
    4. Squeeze æ“ä½œ (static axes only)
    5. Reshape ç¯€é» (no 6Dâ†’4D)
    6. Opset version (recommend 14)
"""

import onnx
from onnx import numpy_helper
import argparse
from pathlib import Path
from collections import defaultdict


class STM32N6Diagnostics:
    """STM32N6 ONNX å…¼å®¹æ€§è¨ºæ–·"""

    def __init__(self, onnx_path):
        self.onnx_path = Path(onnx_path)
        self.model = onnx.load(str(self.onnx_path))
        self.graph = self.model.graph
        self.violations = []
        self.warnings = []

    def diagnose_all(self):
        """åŸ·è¡Œæ‰€æœ‰è¨ºæ–·æª¢æŸ¥"""
        print("="*70)
        print(f"STM32N6 ONNX è¨ºæ–·å ±å‘Š")
        print("="*70)
        print(f"æ¨¡å‹: {self.onnx_path.name}")
        print(f"Opset: {self.model.opset_import[0].version}")
        print("="*70)

        # åŸ·è¡Œæ‰€æœ‰æª¢æŸ¥
        self.check_opset()
        self.check_input_output_shapes()
        self.check_tensor_ranks()
        self.check_dynamic_shapes()
        self.check_reshape_nodes()
        self.check_squeeze_nodes()
        self.check_batch_dimensions()
        self.check_unsupported_ops()

        # ç”Ÿæˆå ±å‘Š
        self.print_report()

        return len(self.violations) == 0

    def check_opset(self):
        """æª¢æŸ¥ Opset ç‰ˆæœ¬"""
        opset = self.model.opset_import[0].version

        if opset == 14:
            print(f"\nâœ… Opset Version: {opset} (RECOMMENDED)")
        elif opset <= 14:
            print(f"\nâš ï¸  Opset Version: {opset} (OK, but 14 is better)")
            self.warnings.append(f"Opset {opset} is old, recommend upgrading to 14")
        else:
            print(f"\nâŒ Opset Version: {opset} (TOO NEW, use 14)")
            self.violations.append(f"Opset {opset} > 14 may not be supported")

    def check_input_output_shapes(self):
        """æª¢æŸ¥è¼¸å…¥è¼¸å‡ºå½¢ç‹€"""
        print(f"\n{'â”€'*70}")
        print("ğŸ“¥ Input/Output Shape Analysis")
        print(f"{'â”€'*70}")

        # æª¢æŸ¥è¼¸å…¥
        for input_tensor in self.graph.input:
            shape = self._get_shape(input_tensor.type.tensor_type.shape)
            rank = len([s for s in shape if s != '?'])

            print(f"\nInput '{input_tensor.name}':")
            print(f"  Shape: {shape}")
            print(f"  Rank: {rank}")

            # Check 1: Rank <= 5
            if rank > 5:
                self.violations.append(
                    f"Input '{input_tensor.name}' has rank {rank} > 5 (6D not supported)"
                )
                print(f"  âŒ VIOLATION: Rank > 5")
            elif rank > 4:
                self.warnings.append(
                    f"Input '{input_tensor.name}' has rank {rank} (5D is max, 4D is safer)"
                )
                print(f"  âš ï¸  WARNING: Rank = 5 (4D is safer)")
            else:
                print(f"  âœ… Rank OK")

            # Check 2: Dynamic batch
            has_dynamic = self._check_dynamic_dims(input_tensor.type.tensor_type.shape)
            if has_dynamic:
                self.violations.append(
                    f"Input '{input_tensor.name}' has dynamic dimensions"
                )
                print(f"  âŒ VIOLATION: Dynamic dimensions detected")
            else:
                print(f"  âœ… All dimensions are fixed")

            # Check 3: Batch = 1
            if shape and shape[0] != 1 and shape[0] != '?':
                self.violations.append(
                    f"Input '{input_tensor.name}' has batch={shape[0]} (must be 1)"
                )
                print(f"  âŒ VIOLATION: Batch = {shape[0]} (must be 1)")
            elif shape and shape[0] == 1:
                print(f"  âœ… Batch = 1")

        # æª¢æŸ¥è¼¸å‡º
        for output_tensor in self.graph.output:
            shape = self._get_shape(output_tensor.type.tensor_type.shape)
            print(f"\nOutput '{output_tensor.name}': {shape}")

            has_dynamic = self._check_dynamic_dims(output_tensor.type.tensor_type.shape)
            if has_dynamic:
                self.violations.append(
                    f"Output '{output_tensor.name}' has dynamic dimensions"
                )
                print(f"  âŒ VIOLATION: Dynamic dimensions")

    def check_tensor_ranks(self):
        """æª¢æŸ¥æ‰€æœ‰ä¸­é–“å¼µé‡çš„ rank"""
        print(f"\n{'â”€'*70}")
        print("ğŸ” Intermediate Tensor Rank Analysis")
        print(f"{'â”€'*70}")

        rank_stats = defaultdict(list)

        # æª¢æŸ¥æ‰€æœ‰ value_infoï¼ˆä¸­é–“å¼µé‡ï¼‰
        for value_info in self.graph.value_info:
            if value_info.type.tensor_type.HasField('shape'):
                shape = self._get_shape(value_info.type.tensor_type.shape)
                rank = len([s for s in shape if s != '?'])
                rank_stats[rank].append((value_info.name, shape))

                if rank > 5:
                    self.violations.append(
                        f"Tensor '{value_info.name}' has rank {rank} > 5: {shape}"
                    )

        # é¡¯ç¤ºçµ±è¨ˆ
        print(f"\nTensor Rank Distribution:")
        for rank in sorted(rank_stats.keys()):
            tensors = rank_stats[rank]
            status = "âŒ" if rank > 5 else "âš ï¸" if rank == 5 else "âœ…"
            print(f"  {status} Rank {rank}: {len(tensors)} tensors")

            # é¡¯ç¤º 6D å¼µé‡è©³æƒ…
            if rank > 5:
                print(f"     6D Tensors (VIOLATIONS):")
                for name, shape in tensors[:5]:  # æœ€å¤šé¡¯ç¤º 5 å€‹
                    print(f"       - {name}: {shape}")
                if len(tensors) > 5:
                    print(f"       - ... and {len(tensors) - 5} more")

    def check_dynamic_shapes(self):
        """æª¢æŸ¥æ‰€æœ‰å‹•æ…‹å½¢ç‹€"""
        print(f"\n{'â”€'*70}")
        print("ğŸ”€ Dynamic Shape Analysis")
        print(f"{'â”€'*70}")

        dynamic_count = 0

        # æª¢æŸ¥æ‰€æœ‰ç¯€é»çš„è¼¸å‡º
        for node in self.graph.node:
            # æª¢æŸ¥ Constant ç¯€é»ï¼ˆå¯èƒ½åŒ…å« shape åƒæ•¸ï¼‰
            if node.op_type == 'Constant':
                for attr in node.attribute:
                    if attr.name == 'value' and attr.HasField('t'):
                        tensor = attr.t
                        if tensor.data_type == onnx.TensorProto.INT64:
                            if tensor.raw_data:
                                data = numpy_helper.to_array(tensor)
                                if any(data == -1) or any(data == 0):
                                    dynamic_count += 1
                                    self.violations.append(
                                        f"Constant node '{node.output[0]}' contains dynamic shape: {data.tolist()}"
                                    )
                                    print(f"  âŒ {node.output[0]}: {data.tolist()}")

        if dynamic_count == 0:
            print(f"  âœ… No dynamic shape constants detected")
        else:
            print(f"  âŒ Found {dynamic_count} dynamic shape constants")

    def check_reshape_nodes(self):
        """æª¢æŸ¥ Reshape ç¯€é»ï¼ˆå°‹æ‰¾ 6Dâ†’4Dï¼‰"""
        print(f"\n{'â”€'*70}")
        print("ğŸ”„ Reshape Node Analysis")
        print(f"{'â”€'*70}")

        reshape_count = 0

        for node in self.graph.node:
            if node.op_type == 'Reshape':
                reshape_count += 1

                # å˜—è©¦æ‰¾åˆ° shape åƒæ•¸
                shape_input = node.input[1] if len(node.input) > 1 else None

                if shape_input:
                    # æŸ¥æ‰¾å°æ‡‰çš„ Constant ç¯€é»
                    shape_value = self._find_constant_value(shape_input)
                    if shape_value is not None:
                        print(f"\n  Reshape '{node.output[0]}':")
                        print(f"    Shape input: {shape_input}")
                        print(f"    Target shape: {shape_value.tolist()}")

                        # æª¢æŸ¥æ˜¯å¦åŒ…å« 6D
                        if len(shape_value) > 5:
                            self.violations.append(
                                f"Reshape to 6D shape detected: {shape_value.tolist()}"
                            )
                            print(f"    âŒ VIOLATION: Reshape to 6D")
                        elif any(shape_value == -1):
                            self.warnings.append(
                                f"Reshape with dynamic dimension (-1): {shape_value.tolist()}"
                            )
                            print(f"    âš ï¸  WARNING: Dynamic dimension (-1)")
                        else:
                            print(f"    âœ… OK")

        print(f"\n  Total Reshape nodes: {reshape_count}")

    def check_squeeze_nodes(self):
        """æª¢æŸ¥ Squeeze ç¯€é»ï¼ˆaxes å¿…é ˆæ˜¯éœæ…‹å±¬æ€§ï¼‰"""
        print(f"\n{'â”€'*70}")
        print("ğŸ—œï¸  Squeeze/Unsqueeze Node Analysis")
        print(f"{'â”€'*70}")

        for op_type in ['Squeeze', 'Unsqueeze']:
            nodes = [n for n in self.graph.node if n.op_type == op_type]

            if not nodes:
                print(f"\n  {op_type}: None")
                continue

            print(f"\n  {op_type} nodes: {len(nodes)}")

            for node in nodes:
                # Opset 13+: axes å¯èƒ½æ˜¯ç¬¬äºŒå€‹è¼¸å…¥
                if len(node.input) > 1:
                    axes_input = node.input[1]
                    axes_value = self._find_constant_value(axes_input)

                    if axes_value is None:
                        self.violations.append(
                            f"{op_type} node '{node.output[0]}' uses tensor input for axes (not supported)"
                        )
                        print(f"    âŒ '{node.output[0]}': axes from tensor (VIOLATION)")
                    else:
                        print(f"    âœ… '{node.output[0]}': axes = {axes_value.tolist()}")

                # Opset 11-12: axes å¯èƒ½æ˜¯å±¬æ€§
                else:
                    has_axes_attr = any(attr.name == 'axes' for attr in node.attribute)
                    if has_axes_attr:
                        axes = [attr.ints for attr in node.attribute if attr.name == 'axes'][0]
                        print(f"    âœ… '{node.output[0]}': axes = {list(axes)} (attribute)")
                    else:
                        print(f"    âš ï¸  '{node.output[0]}': no explicit axes")

    def check_batch_dimensions(self):
        """æª¢æŸ¥ Conv/Pooling å±¤çš„ batch ç¶­åº¦"""
        print(f"\n{'â”€'*70}")
        print("ğŸ”¢ Batch Dimension Analysis (Conv/Pool layers)")
        print(f"{'â”€'*70}")

        conv_ops = ['Conv', 'ConvTranspose', 'MaxPool', 'AveragePool', 'GlobalAveragePool']

        batch_violations = []

        for node in self.graph.node:
            if any(node.op_type.startswith(op) for op in conv_ops):
                # å˜—è©¦æ‰¾åˆ°è¼¸å…¥å¼µé‡çš„å½¢ç‹€
                input_name = node.input[0]
                input_shape = self._find_tensor_shape(input_name)

                if input_shape:
                    batch = input_shape[0] if input_shape else None

                    if batch and batch != 1 and batch != '?':
                        batch_violations.append((node.name, node.op_type, input_name, batch))
                        print(f"  âŒ {node.op_type} '{node.name or node.output[0]}':")
                        print(f"       Input: {input_name}, Batch: {batch} (MUST BE 1)")

        if batch_violations:
            self.violations.append(
                f"Found {len(batch_violations)} Conv/Pool layers with batch > 1"
            )
        else:
            print(f"  âœ… All Conv/Pool layers appear to have batch=1 or unknown")

    def check_unsupported_ops(self):
        """æª¢æŸ¥ä¸æ”¯æ´çš„é‹ç®—"""
        print(f"\n{'â”€'*70}")
        print("âš™ï¸  Operation Support Check")
        print(f"{'â”€'*70}")

        # ST å·²çŸ¥ä¸æ”¯æ´çš„é‹ç®—ï¼ˆç¤ºä¾‹åˆ—è¡¨ï¼Œéœ€æ ¹æ“šå¯¦éš›æ–‡æª”æ›´æ–°ï¼‰
        unsupported_ops = {
            'Loop', 'Scan', 'If', 'SequenceConstruct', 'SequenceAt',
            'NonMaxSuppression', 'RoiAlign', 'TopK'
        }

        op_counts = defaultdict(int)
        for node in self.graph.node:
            op_counts[node.op_type] += 1

        print(f"\n  Operations used:")
        for op_type in sorted(op_counts.keys()):
            count = op_counts[op_type]
            if op_type in unsupported_ops:
                print(f"    âŒ {op_type}: {count} (UNSUPPORTED)")
                self.violations.append(f"Unsupported operation: {op_type}")
            else:
                print(f"    âœ… {op_type}: {count}")

    def print_report(self):
        """æ‰“å°æœ€çµ‚å ±å‘Š"""
        print(f"\n{'='*70}")
        print("ğŸ“Š FINAL DIAGNOSTIC REPORT")
        print(f"{'='*70}")

        print(f"\nç¸½è¨ˆæª¢æŸ¥é …ç›®:")
        print(f"  - âŒ Violations: {len(self.violations)}")
        print(f"  - âš ï¸  Warnings: {len(self.warnings)}")

        if self.violations:
            print(f"\nâŒ VIOLATIONS (å¿…é ˆä¿®å¾©):")
            for i, v in enumerate(self.violations, 1):
                print(f"  {i}. {v}")

        if self.warnings:
            print(f"\nâš ï¸  WARNINGS (å»ºè­°ä¿®å¾©):")
            for i, w in enumerate(self.warnings, 1):
                print(f"  {i}. {w}")

        print(f"\n{'='*70}")
        if not self.violations:
            print("âœ… MODEL IS STM32N6-COMPATIBLE!")
            print("Ready for stedgeai analyze --model ... --target stm32n6")
        else:
            print("âŒ MODEL HAS VIOLATIONS - MUST FIX BEFORE STM32 DEPLOYMENT")
            print("Use fix_onnx_for_stm32.py to repair the model")
        print(f"{'='*70}")

    # Helper methods

    def _get_shape(self, tensor_shape):
        """ç²å–å¼µé‡å½¢ç‹€ï¼ˆåŒ…æ‹¬å‹•æ…‹ç¶­åº¦ï¼‰"""
        shape = []
        for dim in tensor_shape.dim:
            if dim.HasField('dim_value'):
                shape.append(dim.dim_value)
            elif dim.HasField('dim_param'):
                shape.append(f'?({dim.dim_param})')
            else:
                shape.append('?')
        return shape

    def _check_dynamic_dims(self, tensor_shape):
        """æª¢æŸ¥æ˜¯å¦æœ‰å‹•æ…‹ç¶­åº¦"""
        for dim in tensor_shape.dim:
            if dim.HasField('dim_param'):
                return True
        return False

    def _find_constant_value(self, name):
        """æŸ¥æ‰¾ Constant ç¯€é»çš„å€¼"""
        for node in self.graph.node:
            if node.op_type == 'Constant' and node.output[0] == name:
                for attr in node.attribute:
                    if attr.name == 'value' and attr.HasField('t'):
                        return numpy_helper.to_array(attr.t)
        return None

    def _find_tensor_shape(self, name):
        """æŸ¥æ‰¾å¼µé‡çš„å½¢ç‹€"""
        # æª¢æŸ¥ value_info
        for value_info in self.graph.value_info:
            if value_info.name == name and value_info.type.tensor_type.HasField('shape'):
                return self._get_shape(value_info.type.tensor_type.shape)

        # æª¢æŸ¥ input
        for input_tensor in self.graph.input:
            if input_tensor.name == name:
                return self._get_shape(input_tensor.type.tensor_type.shape)

        return None


def main():
    parser = argparse.ArgumentParser(description='STM32N6 ONNX Diagnostics')
    parser.add_argument('--onnx', type=str, required=True, help='Path to ONNX model')
    args = parser.parse_args()

    # åŸ·è¡Œè¨ºæ–·
    diagnostics = STM32N6Diagnostics(args.onnx)
    is_compatible = diagnostics.diagnose_all()

    # è¿”å›ç‹€æ…‹ç¢¼
    exit(0 if is_compatible else 1)


if __name__ == "__main__":
    main()
